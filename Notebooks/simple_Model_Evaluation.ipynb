{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "- So far, learned about two classifiers:\n",
    "\n",
    "    - SVM\n",
    "    \n",
    "    - Logistic Regression\n",
    "    \n",
    "- How we can evalute which model performs better for our classification task?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets review the confusion matrix we obtained from logistic regression model for diabetes dataset\n",
    "\n",
    "<img src=\"Images/confusion_matrix.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is accuracy, recall, precision, Specificity and F1-score?\n",
    "\n",
    "- Accuracy, recall, precision, Specificity and F1-score are all obtained from Confision Matrix elements known as : TN, FP, FN, TP\n",
    "\n",
    "Accuracy: overall, how often is the classifier correct? -> $accuracy = \\frac {TP + TN}{TP+TN+FP+FN}$\n",
    "\n",
    "Classification error: overall, how often is the classifier incorrect? -> $error = 1- accuracy = \\frac {FP + FN}{TP + TN + FP + FN}$\n",
    "\n",
    "Recall: when the actual value is positive, how often is the prediction correct? -> $recall = \\frac {TP}{TP + FN}$\n",
    "\n",
    "Precision: When a positive value is predicted, how often is the prediction correct? -> $precision = \\frac {TP}{TP + FP}$\n",
    "\n",
    "Specificity: When the actual value is negative, how often is the prediction correct? -> $Specificity = \\frac {TN}{TN + FP}$\n",
    "\n",
    "F1-score = 2 x (Precision x Recall )/(Precision + Recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity:  Write a function that returns Accuray, Precision, Recall, F1-score from Confision Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6927083333333334, 0.5555555555555556, 0.24193548387096775, 0.3370786516853933)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "# print(confusion)\n",
    "\n",
    "confusion = np.array([[118., 12.], [ 47., 15.]])\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "TP = confusion[1, 1]\n",
    "\n",
    "Accuray = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "\n",
    "Precision = TP / float(TP + FP)\n",
    "\n",
    "Recall = TP / float(FN + TP)\n",
    "\n",
    "F1_score = 2*Precision*Recall/float(Precision+Recall)\n",
    "\n",
    "print((Accuray, Precision, Recall, F1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity (Reminder): How we can obtain y_pred (whose elements are 0 and 1) from :\n",
    "\n",
    "- `y_pred_prob = logreg.predict_proba(X_test)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Change the threshold of Logistic Regression (y_pred_prob) to get new confusion matrix and better model performance\n",
    "\n",
    "- The question now is which threshold is better?\n",
    "\n",
    "- To do this:\n",
    "\n",
    "1- Train `logreg = LogisticRegression()` and get `y_pred_prob = logreg.predict_proba(X_test)`\n",
    "\n",
    "2- The second column of `y_pred_prob` is the probability that a subject be diabetes \n",
    "\n",
    "3- Plot histogram of second column. Hint: `plt.hist(y_pred_prob[:, 1], bins=8) plt.show()`\n",
    "\n",
    "4- Count how many of `y_train` is 0 how many is 1. Define thershold as `threshold = y_train.value_counts()[1] / len(y_train)`\n",
    "\n",
    "5- Write a function that returns 0 if `y_pred_prob[:, 1]` is less than threshold, else returns 1 for all elements in `y_pred_prob`\n",
    "\n",
    "6- Calculate the Confuction Matrix by `confusion = metrics.confusion_matrix(y_test, y_pred)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
      "      dtype='object')\n",
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pima = pd.read_csv('diabetes.csv')\n",
    "print(pima.columns)\n",
    "print(pima.head())\n",
    "\n",
    "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\n",
    "\n",
    "X = pima[feature_cols]\n",
    "# print(X)\n",
    "# y is a vector, hence we use dot to access 'label'\n",
    "y = pima['Outcome']\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = logreg.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63247571, 0.36752429],\n",
       "       [0.71643656, 0.28356344],\n",
       "       [0.71104114, 0.28895886],\n",
       "       [0.5858938 , 0.4141062 ],\n",
       "       [0.84103973, 0.15896027],\n",
       "       [0.82934844, 0.17065156],\n",
       "       [0.50110974, 0.49889026],\n",
       "       [0.48658459, 0.51341541],\n",
       "       [0.72321388, 0.27678612],\n",
       "       [0.32810562, 0.67189438],\n",
       "       [0.64244443, 0.35755557],\n",
       "       [0.25912035, 0.74087965],\n",
       "       [0.63949765, 0.36050235],\n",
       "       [0.76987637, 0.23012363],\n",
       "       [0.57345769, 0.42654231],\n",
       "       [0.80896485, 0.19103515],\n",
       "       [0.54236399, 0.45763601],\n",
       "       [0.8809859 , 0.1190141 ],\n",
       "       [0.56071047, 0.43928953],\n",
       "       [0.63038849, 0.36961151],\n",
       "       [0.55812011, 0.44187989],\n",
       "       [0.62388338, 0.37611662],\n",
       "       [0.80183978, 0.19816022],\n",
       "       [0.58322696, 0.41677304],\n",
       "       [0.84451719, 0.15548281],\n",
       "       [0.7468329 , 0.2531671 ],\n",
       "       [0.90256923, 0.09743077],\n",
       "       [0.30366288, 0.69633712],\n",
       "       [0.84641691, 0.15358309],\n",
       "       [0.7802164 , 0.2197836 ],\n",
       "       [0.56905168, 0.43094832],\n",
       "       [0.65783942, 0.34216058],\n",
       "       [0.77603886, 0.22396114],\n",
       "       [0.61926457, 0.38073543],\n",
       "       [0.86657866, 0.13342134],\n",
       "       [0.61209784, 0.38790216],\n",
       "       [0.52950297, 0.47049703],\n",
       "       [0.83795257, 0.16204743],\n",
       "       [0.70451824, 0.29548176],\n",
       "       [0.69081839, 0.30918161],\n",
       "       [0.72700295, 0.27299705],\n",
       "       [0.61183417, 0.38816583],\n",
       "       [0.72646557, 0.27353443],\n",
       "       [0.71118959, 0.28881041],\n",
       "       [0.36528086, 0.63471914],\n",
       "       [0.97634749, 0.02365251],\n",
       "       [0.84179352, 0.15820648],\n",
       "       [0.76981625, 0.23018375],\n",
       "       [0.6515407 , 0.3484593 ],\n",
       "       [0.72419959, 0.27580041],\n",
       "       [0.66735896, 0.33264104],\n",
       "       [0.75119404, 0.24880596],\n",
       "       [0.25510488, 0.74489512],\n",
       "       [0.60998536, 0.39001464],\n",
       "       [0.58374455, 0.41625545],\n",
       "       [0.86424313, 0.13575687],\n",
       "       [0.81104624, 0.18895376],\n",
       "       [0.35222318, 0.64777682],\n",
       "       [0.81077869, 0.18922131],\n",
       "       [0.94314096, 0.05685904],\n",
       "       [0.36008453, 0.63991547],\n",
       "       [0.53363618, 0.46636382],\n",
       "       [0.8749028 , 0.1250972 ],\n",
       "       [0.73042398, 0.26957602],\n",
       "       [0.75080896, 0.24919104],\n",
       "       [0.69429604, 0.30570396],\n",
       "       [0.53623776, 0.46376224],\n",
       "       [0.79036905, 0.20963095],\n",
       "       [0.57152171, 0.42847829],\n",
       "       [0.59237736, 0.40762264],\n",
       "       [0.79830904, 0.20169096],\n",
       "       [0.72972934, 0.27027066],\n",
       "       [0.73744144, 0.26255856],\n",
       "       [0.42761737, 0.57238263],\n",
       "       [0.54532959, 0.45467041],\n",
       "       [0.72283848, 0.27716152],\n",
       "       [0.41998719, 0.58001281],\n",
       "       [0.58400512, 0.41599488],\n",
       "       [0.72723899, 0.27276101],\n",
       "       [0.65900777, 0.34099223],\n",
       "       [0.45373422, 0.54626578],\n",
       "       [0.62069277, 0.37930723],\n",
       "       [0.7007795 , 0.2992205 ],\n",
       "       [0.89940831, 0.10059169],\n",
       "       [0.67127398, 0.32872602],\n",
       "       [0.54898637, 0.45101363],\n",
       "       [0.83963021, 0.16036979],\n",
       "       [0.5103025 , 0.4896975 ],\n",
       "       [0.36769492, 0.63230508],\n",
       "       [0.59261596, 0.40738404],\n",
       "       [0.80205603, 0.19794397],\n",
       "       [0.80301979, 0.19698021],\n",
       "       [0.75536792, 0.24463208],\n",
       "       [0.88852815, 0.11147185],\n",
       "       [0.5841403 , 0.4158597 ],\n",
       "       [0.78438144, 0.21561856],\n",
       "       [0.45875471, 0.54124529],\n",
       "       [0.51196398, 0.48803602],\n",
       "       [0.35347233, 0.64652767],\n",
       "       [0.66059342, 0.33940658],\n",
       "       [0.45736573, 0.54263427],\n",
       "       [0.83786176, 0.16213824],\n",
       "       [0.6221259 , 0.3778741 ],\n",
       "       [0.88688713, 0.11311287],\n",
       "       [0.65218013, 0.34781987],\n",
       "       [0.65957216, 0.34042784],\n",
       "       [0.8209015 , 0.1790985 ],\n",
       "       [0.78675188, 0.21324812],\n",
       "       [0.85289054, 0.14710946],\n",
       "       [0.76985898, 0.23014102],\n",
       "       [0.81595408, 0.18404592],\n",
       "       [0.47775351, 0.52224649],\n",
       "       [0.52900634, 0.47099366],\n",
       "       [0.71115752, 0.28884248],\n",
       "       [0.50674921, 0.49325079],\n",
       "       [0.58255527, 0.41744473],\n",
       "       [0.77084992, 0.22915008],\n",
       "       [0.72977089, 0.27022911],\n",
       "       [0.80756076, 0.19243924],\n",
       "       [0.2501287 , 0.7498713 ],\n",
       "       [0.53499907, 0.46500093],\n",
       "       [0.3354546 , 0.6645454 ],\n",
       "       [0.57901401, 0.42098599],\n",
       "       [0.46435966, 0.53564034],\n",
       "       [0.83965298, 0.16034702],\n",
       "       [0.8564314 , 0.1435686 ],\n",
       "       [0.61857574, 0.38142426],\n",
       "       [0.66172686, 0.33827314],\n",
       "       [0.6369935 , 0.3630065 ],\n",
       "       [0.87157469, 0.12842531],\n",
       "       [0.71666307, 0.28333693],\n",
       "       [0.95994442, 0.04005558],\n",
       "       [0.81518861, 0.18481139],\n",
       "       [0.33283053, 0.66716947],\n",
       "       [0.53647126, 0.46352874],\n",
       "       [0.51284318, 0.48715682],\n",
       "       [0.80089206, 0.19910794],\n",
       "       [0.54138349, 0.45861651],\n",
       "       [0.76783279, 0.23216721],\n",
       "       [0.81630733, 0.18369267],\n",
       "       [0.73608006, 0.26391994],\n",
       "       [0.62507031, 0.37492969],\n",
       "       [0.87083494, 0.12916506],\n",
       "       [0.58586087, 0.41413913],\n",
       "       [0.57539142, 0.42460858],\n",
       "       [0.86167809, 0.13832191],\n",
       "       [0.79218306, 0.20781694],\n",
       "       [0.70522301, 0.29477699],\n",
       "       [0.84174901, 0.15825099],\n",
       "       [0.63983766, 0.36016234],\n",
       "       [0.76258551, 0.23741449],\n",
       "       [0.56649311, 0.43350689],\n",
       "       [0.79380119, 0.20619881],\n",
       "       [0.76837662, 0.23162338],\n",
       "       [0.38888459, 0.61111541],\n",
       "       [0.80268991, 0.19731009],\n",
       "       [0.19928502, 0.80071498],\n",
       "       [0.82191509, 0.17808491],\n",
       "       [0.63511265, 0.36488735],\n",
       "       [0.21381357, 0.78618643],\n",
       "       [0.55919386, 0.44080614],\n",
       "       [0.63440346, 0.36559654],\n",
       "       [0.88239862, 0.11760138],\n",
       "       [0.77156675, 0.22843325],\n",
       "       [0.52134931, 0.47865069],\n",
       "       [0.78679475, 0.21320525],\n",
       "       [0.48501479, 0.51498521],\n",
       "       [0.83877506, 0.16122494],\n",
       "       [0.76259881, 0.23740119],\n",
       "       [0.70625609, 0.29374391],\n",
       "       [0.83329952, 0.16670048],\n",
       "       [0.51283474, 0.48716526],\n",
       "       [0.70030106, 0.29969894],\n",
       "       [0.55348957, 0.44651043],\n",
       "       [0.49830098, 0.50169902],\n",
       "       [0.70753494, 0.29246506],\n",
       "       [0.38263772, 0.61736228],\n",
       "       [0.58406005, 0.41593995],\n",
       "       [0.74179055, 0.25820945],\n",
       "       [0.8258032 , 0.1741968 ],\n",
       "       [0.66480459, 0.33519541],\n",
       "       [0.30393175, 0.69606825],\n",
       "       [0.67545632, 0.32454368],\n",
       "       [0.64269574, 0.35730426],\n",
       "       [0.7663053 , 0.2336947 ],\n",
       "       [0.76261476, 0.23738524],\n",
       "       [0.61590682, 0.38409318],\n",
       "       [0.75308588, 0.24691412],\n",
       "       [0.72045448, 0.27954552],\n",
       "       [0.81498826, 0.18501174],\n",
       "       [0.7377638 , 0.2622362 ],\n",
       "       [0.72143074, 0.27856926]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.65625, 0.4791666666666667, 0.7419354838709677, 0.5822784810126582)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "confusion = np.array([[80, 50], [16, 46]])\n",
    "\n",
    "# confusion = np.array([[87, 43], [24, 38]])\n",
    "\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "TP = confusion[1, 1]\n",
    "\n",
    "Accuray = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "\n",
    "Precision = TP / float(TP + FP)\n",
    "\n",
    "Recall = TP / float(FN + TP)\n",
    "\n",
    "F1_score = 2*Precision*Recall/float(Precision+Recall)\n",
    "\n",
    "print((Accuray, Precision, Recall, F1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "- With threshold 0.5 -> `confusion = np.array([[118., 12.], [ 47., 15.]])`\n",
    "\n",
    "- With optimal threshold -> `confusion = np.array([[80, 50], [16, 46]])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With threshold 0.5\n",
    "print((Accuray, Precision, Recall, F1_score))\n",
    "\n",
    "(0.6927083333333334, 0.5555555555555556, 0.24193548387096775, 0.3370786516853933)\n",
    "\n",
    "# With Optimal Theshold\n",
    "print((Accuray, Precision, Recall, F1_score))\n",
    "\n",
    "(0.65625, 0.4791666666666667, 0.7419354838709677, 0.5822784810126582)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "- Normally in a machine learning process, data is divided into training and test sets; the training set is then used to train the model and the test set is used to evaluate the performance of a model \n",
    "\n",
    "- It is possible that the accuracy obtained on one test is very different to accuracy obtained on another test set using the same algorithm\n",
    "\n",
    "- To see the model performance, we use K-Fold Cross-Validation for performance evaluation where K is any number\n",
    "\n",
    "- Suppose we want to perform 5-fold cross validation\n",
    "\n",
    "\n",
    "<img src=\"Images/cross_validation.png\" width=\"500\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets obtain Accuracy and F1-Score for 5-fold cross validation based on diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "268\n",
      "[0.64935065 0.65584416 0.64935065 0.69281046 0.65359477]\n",
      "0.6601901366607248\n",
      "[0.578125   0.55462185 0.54237288 0.624      0.576     ]\n",
      "0.5750239460190857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pima = pd.read_csv('diabetes.csv')\n",
    "\n",
    "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\n",
    "\n",
    "# X is a matrix,access the features we want in feature_cols\n",
    "X = pima[feature_cols]\n",
    "\n",
    "# y is a vector, hence we use dot to access 'label'\n",
    "y = pima['Outcome']\n",
    "\n",
    "print(y.value_counts()[0])\n",
    "print(y.value_counts()[1])\n",
    "\n",
    "logreg = LogisticRegression(class_weight={1: 500/268})\n",
    "#logreg = LogisticRegression(class_weight={1: y.value_counts()[0]/y.value_counts()[1]})\n",
    "# logreg = LogisticRegression()\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=logreg, X=X, y=y, cv=5, scoring='accuracy')\n",
    "print(all_accuracies)\n",
    "print(all_accuracies.mean())\n",
    "\n",
    "all_f1 = cross_val_score(estimator=logreg, X=X, y=y, cv=5, scoring='f1')\n",
    "print(all_f1)\n",
    "print(all_f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.hist(y, bins=20)\n",
    "plt.show()\n",
    "\n",
    "y_pd_series = pd.Series(y)\n",
    "y_pd_series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose Models based on Cross-Validation\n",
    "\n",
    "- We want to have low variance result for CV -> pick a model that has lower variance\n",
    "\n",
    "- If two models have low variance result for CV -> pick a model that has higher mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Parameter Selection\n",
    "\n",
    "- Machine learning models have hyper-parameters. To know which values of hyper-paramaeters give the best result we need grid search\n",
    "\n",
    "- Question: what does grid search mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid Search for Parameter Selection\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
