{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/naive_slide_1.png\" width=\"700\" height=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/naive_slide_2.png\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "Text classification is to attach labels to bodies of text, e.g., tax document, medical form, etc. based on the text itself\n",
    "\n",
    "Think of your spam folder in your email. How does your email provider know that a particular message is spam or “ham” (not spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the pre-processings to apply a machine learning algorithm on text data?\n",
    "\n",
    "1. The text must be parsed to words, called tokenization\n",
    "\n",
    "2. Then the words need to be encoded as integers or floating point values\n",
    "\n",
    "3. scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Activity: Apply Bag-of Word (BoW) to text dataset\n",
    "\n",
    "BoW model is simple. It throws away all of the order information in the words and focuses on the occurrence of words in a document\n",
    "\n",
    "Apply BoW with `CountVectorizer` in sklearn to the following `corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "'This document is the second document.',\n",
    "'And this is the third one.',\n",
    "'Is this the first document?']\n",
    "# vectorizer = TfidfVectorizer()\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies with TfidfVectorizer\n",
    "\n",
    "Word counts are a good starting point, but are very basic\n",
    "\n",
    "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. \n",
    "\n",
    "**Term Frequency**: This summarizes how often a given word appears within a document\n",
    "\n",
    "**Inverse Document Frequency**: This downscales words that appear a lot across documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitive idea behind TF-IDF:\n",
    "    \n",
    "- If a word appears frequently in a document, it's important. Give the word a high score\n",
    "\n",
    "- But if a word appears in many documents, it's not a unique identifier. Give the word a low score\n",
    "\n",
    "<img src=\"Images/tfidf_slide.png\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF in Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "'This document is the second document.',\n",
    "'And this is the third one.',\n",
    "'Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Reading: Use Keras for text pre-processing\n",
    "\n",
    "Another package that has good methods for tokenization and BoW, TF-IDF, ... is Keras\n",
    "\n",
    "Keras is a high-level neural networks API (Deep Learning API), written in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.58778666 0.58778666 0.58778666 0.69314718 0.84729786\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.58778666 0.58778666 0.58778666 1.17360019 0.\n",
      "  1.09861229 0.         0.         0.        ]\n",
      " [0.         0.58778666 0.58778666 0.58778666 0.         0.\n",
      "  0.         1.09861229 1.09861229 1.09861229]\n",
      " [0.         0.58778666 0.58778666 0.58778666 0.69314718 0.84729786\n",
      "  0.         0.         0.         0.        ]]\n",
      "[[1, 2, 3, 5, 4], [1, 4, 2, 3, 6, 4], [7, 1, 2, 3, 8, 9], [2, 1, 3, 5, 4]]\n",
      "{'this': 1, 'is': 2, 'the': 3, 'document': 4, 'first': 5, 'second': 6, 'and': 7, 'third': 8, 'one': 9}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "# tok = Tokenizer(num_words=20)\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(corpus)\n",
    "BoW = tok.texts_to_matrix(corpus, mode='tfidf')\n",
    "print(BoW)\n",
    "X = tok.texts_to_sequences(corpus)\n",
    "print(X)\n",
    "print(tok.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier (Math)\n",
    "\n",
    "The Bayes Theorem : $P(spam | w_1, w_2, ..., w_n) = {P(w_1, w_2, ..., w_n | spam)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "Naive Bayes assumption is that each word is independent of all other words, In reality, this is not true! But lets try it out for real world examples\n",
    "\n",
    "So the above relations become simple with this assumption: $P(spam | w_1, w_2, ..., w_n) = {P(w_1| spam)P(w_2| spam) ... P(w_n| spam)P(spam)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "Taking log from both side and the denuminator is independent of spam or ham:\n",
    "\n",
    "$logP(spam | w_1, w_2, ..., w_n) \\propto {\\sum_{i=1}^{n}log P(w_i| spam)+ log P(spam)}$\n",
    "\n",
    "Also:\n",
    "\n",
    "$logP(ham | w_1, w_2, ..., w_n) \\propto {\\sum_{i=1}^{n}log P(w_i| ham)+ log P(ham)}$\n",
    "\n",
    "\n",
    "if \n",
    "\n",
    "${\\sum_{i=1}^{n}log P(w_i| spam)+ log P(spam)} > {\\sum_{i=1}^{n}log P(w_i| ham)+ log P(ham)}$\n",
    "\n",
    "then that sentences is spam \n",
    "\n",
    "else\n",
    "\n",
    "the sentences is ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-code for Naive Bayes for spam/ham dataset:\n",
    "\n",
    "- Assume the following small dataset is given\n",
    "\n",
    "- The first column is the labels of received emails\n",
    "\n",
    "- The second column is the sentenses, body of the email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/spam_minidataset.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Based on the given dataset above, create the following two dictionaries:\n",
    "\n",
    "    - Ham -> D_ham = {'Jos': 1,'ask':1, 'you':1,... }\n",
    "    \n",
    "    - Spam- > D_spam= {'Did': 1, 'you':3, ... }\n",
    "    \n",
    "Each dictionary representes all words for the spam and ham emails and their occurance numbers (as the value of dictionaries)\n",
    "\n",
    "2- For any new given sentenses, having $w_1$, $w_2$, ... $w_n$ words, assume the sentences is ham, calculate:\n",
    "\n",
    " $P(w_1| ham)$, $P(w_2| ham)$, ..., $P(w_n| ham)$\n",
    " \n",
    " then $log(P(w_1| ham))$, $log(P(w_2| ham))$, ..., $log(P(w_n| ham))$\n",
    " \n",
    "add them all \n",
    "\n",
    "\n",
    "3- Calculate what percentage of labels is ham -> $P(ham)$ -> take log -> $log(P(ham))$\n",
    "\n",
    "4- Add the value from step (2) and (3)\n",
    "\n",
    "5- Do Steps (2) - (4), and now assume the given new sentences is spam\n",
    "\n",
    "6- Compare the two values, the greater one would be the class\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply the naive Bayes to spam/ham email dataset:\n",
    "\n",
    "Please read this article: https://pythonmachinelearning.pro/text-classification-tutorial-with-naive-bayes/\n",
    "\n",
    "```\n",
    "data = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Naive Bayes core parts in the following Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5572\n",
      "{'spam': 567, 'ham': 3612}\n",
      "0.9641\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit our classifier\n",
    "        Arguments:\n",
    "            X {list} -- list of document contents\n",
    "            y {list} -- correct labels\n",
    "        \"\"\"\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 'spam')\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 'ham')\n",
    "        self.log_class_priors['spam'] = math.log(self.num_messages['spam'] / n )\n",
    "        self.log_class_priors['ham'] = math.log(self.num_messages['ham'] / n )\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 'spam' else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "                \n",
    "                # add Laplace smoothing\n",
    "                log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    "\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "\n",
    "            if spam_score > ham_score:\n",
    "                result.append('spam')\n",
    "            else:\n",
    "                result.append('ham')\n",
    "        return result\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    data = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "    data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "    data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "    print(data.head())\n",
    "    tags = data[\"label\"]\n",
    "    texts = data[\"text\"]\n",
    "    X, y = texts, tags\n",
    "    print(len(X))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    MNB = SpamDetector()\n",
    "    MNB.fit(X_train.values, y_train.values)\n",
    "    print(MNB.num_messages)\n",
    "#     print(MNB.word_counts)\n",
    "    pred = MNB.predict(X_test.values)\n",
    "    true = y_test.values\n",
    "    accuracy = sum(1 for i in range(len(pred)) if pred[i] == true[i]) / float(len(pred))\n",
    "    print(\"{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: use sklearn CountVectorizer and MultinomialNB to spam email dataset\n",
    "\n",
    "Read this blog first: https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/\n",
    "\n",
    "steps:\n",
    "\n",
    "1- Split the dataset\n",
    "\n",
    "`from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)`\n",
    "\n",
    "2- Vectorizing our dataset : `vect = CountVectorizer()`\n",
    "\n",
    "3- Transform training data into a document-term matrix (BoW): `X_train_dtm = vect.fit_transform(X_train)`\n",
    "\n",
    "4- Build and evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# instantiate a Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "  (0, 4527)\t1\n",
      "  (0, 3636)\t1\n",
      "  (0, 3647)\t1\n",
      "  (0, 3493)\t1\n",
      "  (0, 5683)\t1\n",
      "  (0, 3263)\t1\n",
      "  (0, 4469)\t1\n",
      "  (0, 2236)\t1\n",
      "  (0, 4798)\t1\n",
      "  (0, 1488)\t1\n",
      "  (0, 4854)\t1\n",
      "  (0, 3451)\t1\n",
      "  (0, 2218)\t1\n",
      "  (0, 6326)\t2\n",
      "  (0, 6604)\t1\n",
      "  (0, 1535)\t2\n",
      "  (0, 4176)\t2\n",
      "  (0, 7373)\t1\n",
      "  (0, 5065)\t2\n",
      "  (0, 2112)\t1\n",
      "  (0, 6727)\t1\n",
      "  (0, 5712)\t1\n",
      "  (0, 819)\t1\n",
      "  (0, 802)\t1\n",
      "  (0, 919)\t1\n",
      "  :\t:\n",
      "  (4176, 4894)\t1\n",
      "  (4176, 4833)\t1\n",
      "  (4176, 3439)\t1\n",
      "  (4176, 1590)\t1\n",
      "  (4176, 4219)\t1\n",
      "  (4176, 7163)\t1\n",
      "  (4176, 4450)\t1\n",
      "  (4176, 6638)\t1\n",
      "  (4176, 2304)\t1\n",
      "  (4176, 3416)\t1\n",
      "  (4176, 3252)\t1\n",
      "  (4176, 4747)\t1\n",
      "  (4177, 6953)\t1\n",
      "  (4177, 5232)\t1\n",
      "  (4177, 1848)\t1\n",
      "  (4177, 4766)\t1\n",
      "  (4177, 3162)\t1\n",
      "  (4177, 4125)\t1\n",
      "  (4177, 6074)\t1\n",
      "  (4177, 3252)\t1\n",
      "  (4177, 3647)\t1\n",
      "  (4178, 1695)\t1\n",
      "  (4178, 6055)\t1\n",
      "  (4178, 7295)\t1\n",
      "  (4178, 4274)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9856424982053122"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "data = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "\n",
    "X, y = texts, tags\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "print(X_train_dtm)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "- We learned how TF-IDF assign score to the words while considering its frequency in that sentences and in other sentences \n",
    "\n",
    "- Naive Bayes, is the simplest text classification algorithm\n",
    "\n",
    "- Although  it is simple, it works for many applications\n",
    "\n",
    "- To upgrade the text classification performance: Use more advanced models that incorporate (consider) the sentences as the sequence of the words (which word is before X and which word is after X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "\n",
    "- https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
