{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Dictionary:\n",
      "{'send': 3, 'us': 3, 'your': 3, 'password': 2, 'review': 1, 'account': 1}\n",
      "\n",
      "\n",
      "Ham Dictionary:\n",
      "{'send': 1, 'us': 1, 'your': 2, 'review': 2, 'password': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_text= ['Send us your password', 'review us', 'Send your password', 'Send us your account']\n",
    "\n",
    "ham_text= ['Send us your review', 'review your password']\n",
    "\n",
    "spam = {}\n",
    "for i in spam_text:\n",
    "    for j in i.lower().split(' '):\n",
    "        if j not in spam:\n",
    "            spam[j] = 1\n",
    "        else:\n",
    "            spam[j] += 1\n",
    "\n",
    "print(\"Spam Dictionary:\")            \n",
    "print(spam)\n",
    "print(\"\\n\")\n",
    "\n",
    "ham = {}\n",
    "for i in ham_text:\n",
    "    for j in i.lower().split(' '):\n",
    "        if j not in ham:\n",
    "            ham[j] = 1\n",
    "        else:\n",
    "            ham[j] += 1\n",
    "\n",
    "print(\"Ham Dictionary:\")\n",
    "print(ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "p_password_given_spam = spam['password']/sum(spam.values())\n",
    "print(p_password_given_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "# or \n",
    "p_password_given_ham = ham['password']/sum(ham.values())\n",
    "print(p_password_given_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data = pd.read_csv('./datasets/spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "X = data[\"label\"]\n",
    "y = data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit our classifier\n",
    "        Arguments:\n",
    "            X {list} -- list of document contents\n",
    "            y {list} -- correct labels\n",
    "        \"\"\"\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 1)\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 0)\n",
    "        self.log_class_priors['spam'] = math.log(self.num_messages['spam'] / n)\n",
    "        self.log_class_priors['ham'] = math.log(self.num_messages['ham'] / n)\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 1 else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "\n",
    "                # add Laplace smoothing\n",
    "                log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    "\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "\n",
    "            if spam_score > ham_score:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-8b8b25737625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mSD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpamDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mSD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-4dd39432f950>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_messages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spam'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_messages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ham'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_class_priors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spam'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_messages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spam'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_class_priors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ham'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_messages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ham'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spam'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "SD = SpamDetector()\n",
    "SD.fit(X_train, y_train)\n",
    "\n",
    "y_pred = SD.predict(X_test.values)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "  (0, 3286)\t1\n",
      "  (0, 4747)\t2\n",
      "  (0, 1896)\t1\n",
      "  (0, 875)\t2\n",
      "  (0, 6599)\t2\n",
      "  (0, 801)\t1\n",
      "  (0, 5258)\t1\n",
      "  (0, 7209)\t3\n",
      "  (0, 1559)\t1\n",
      "  (0, 913)\t1\n",
      "  (0, 6623)\t3\n",
      "  (0, 1050)\t1\n",
      "  (0, 5980)\t1\n",
      "  (0, 3530)\t1\n",
      "  (0, 919)\t1\n",
      "  (0, 802)\t1\n",
      "  (0, 819)\t1\n",
      "  (0, 5712)\t1\n",
      "  (0, 6727)\t1\n",
      "  (0, 2112)\t1\n",
      "  (0, 5065)\t2\n",
      "  (0, 7373)\t1\n",
      "  (0, 4176)\t2\n",
      "  (0, 1535)\t2\n",
      "  (0, 6604)\t1\n",
      "  :\t:\n",
      "  (4176, 4747)\t1\n",
      "  (4176, 3252)\t1\n",
      "  (4176, 3416)\t1\n",
      "  (4176, 2304)\t1\n",
      "  (4176, 6638)\t1\n",
      "  (4176, 4450)\t1\n",
      "  (4176, 7163)\t1\n",
      "  (4176, 4219)\t1\n",
      "  (4176, 1590)\t1\n",
      "  (4176, 3439)\t1\n",
      "  (4176, 4833)\t1\n",
      "  (4176, 4894)\t1\n",
      "  (4177, 3647)\t1\n",
      "  (4177, 3252)\t1\n",
      "  (4177, 6074)\t1\n",
      "  (4177, 4125)\t1\n",
      "  (4177, 3162)\t1\n",
      "  (4177, 4766)\t1\n",
      "  (4177, 1848)\t1\n",
      "  (4177, 5232)\t1\n",
      "  (4177, 6953)\t1\n",
      "  (4178, 4274)\t1\n",
      "  (4178, 7295)\t1\n",
      "  (4178, 6055)\t1\n",
      "  (4178, 1695)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9856424982053122"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Prepare the dataset\n",
    "data = pd.read_csv('Datasets/spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "\n",
    "# create texts and tags\n",
    "X, y = texts, tags\n",
    "\n",
    "# split the data into train vs test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# transform text into numerical vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "print(X_train_dtm)\n",
    "\n",
    "# instantiate Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "# fit to model, with the trained part of the dataset\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "# make prediction\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "# test accurarcy of prediction\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
